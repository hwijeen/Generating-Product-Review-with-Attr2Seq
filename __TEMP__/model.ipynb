{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "~~1. Batching!~~   \n",
    "    ~~-Encoder.Forward의 input 모양 어떻게 되지? / .view 인자 확인!~~\n",
    "2. Attention  \n",
    "3. Teacher Forcing  \n",
    "~~4. Parameter(things to be updated) 등록 잘 됐나 확인(= 그래프 잘 그린 건지 어케 확인하나?)~~\n",
    "5. Train / Dev 사전에 나누기\n",
    "6. Pretrained word vector을 쓸 수가 있나..?  \n",
    "** SOS, EOS, PAD token 관리 어디서?**   \n",
    "** INTO GPU: x batch, y batch, parameter On GPU**\n",
    "7. model save등등 train 뒷단 얘기  \n",
    "    -torch.save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "1. Decoder가 2 layer일때, initial hidden?  \n",
    "    - https://discuss.pytorch.org/t/understanding-output-of-lstm/12320/2\n",
    "    - hidden 의 dimension\n",
    "2. embedding 거친 후엔 batch_size x seq_len x embedding_dim 임!\n",
    "    - embedding input은 batch_size x seq_len\n",
    "2. LSTM의 batch_first\n",
    "3. LSTM input의 dimension\n",
    "4. NLLLoss()의 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab for tags ...\n",
      "Building vocab for reviews ...\n",
      "3339 out of 3465 words left, which is 96.36363636363636 %\n",
      "21036 out of 37012 words left, which is 56.83562087971469 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b015ad494684c01baa4b68ccbe189eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=163733), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta = build_meta()\n",
    "tagVocab, rvVocab = filter_by_cnt(2, 2, 'data/products.pkl')\n",
    "data = prepareData(meta, tagVocab, rvVocab, 'data/products.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    dropout = 0.2\n",
    "    batch_size = 2\n",
    "    num_steps = 10\n",
    "    \n",
    "    # Encoder\n",
    "    rating_size = len(meta.rating2idx)\n",
    "    category_size = len(meta.lowcat2idx)\n",
    "    tag_size = len(tagVocab.word2idx)    # tag vocab_size\n",
    "    pretrained = False \n",
    "    attribute_size = 64\n",
    "    hidden_size = 512 # fixed-vector size \n",
    "    \n",
    "    # Decoder\n",
    "    num_layers = 2\n",
    "    output_size = len(rvVocab.word2idx)    # review vocab_size\n",
    "    \n",
    "    # TEMP - 이거 어디서 관리하지? 여긴 아닌뎅\n",
    "    padding_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.emb_rating = nn.Embedding(self.config.rating_size, self.config.attribute_size)   \n",
    "        self.emb_category = nn.Embedding(self.config.category_size, self.config.attribute_size)\n",
    "        self.emb_tag = nn.Embedding(self.config.tag_size, self.config.attribute_size,\n",
    "                                   padding_idx=self.config.padding_idx)        \n",
    "        self.out = nn.Linear(self.config.attribute_size * 3, self.config.hidden_size*self.config.num_layers)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def forward(self, rating, category, tag):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            rating: TENSOR of shape (batch_size, 1)\n",
    "            category: TENSOR of shape (batch_size, 1)\n",
    "            tag : 1) TENSOR of shape (batch_size, tag_MAXLEN)\n",
    "        Returns:\n",
    "            concatenated attr for attention, encoder_output\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(rating) == len(category) == len(tag)\n",
    "        attr_rating = self.emb_rating(rating)    # view no need!\n",
    "        attr_category = self.emb_category(category)\n",
    "        tag_len = self.get_tag_len(tag)    \n",
    "        attr_tag = torch.sum(self.emb_tag(tag), 1, keepdim=True) / tag_len    # CBOW\n",
    "                # 여기서 이렇게 해도 backprop 맞게 되나??\n",
    "        \n",
    "        attr = torch.cat((attr_rating, attr_category, attr_tag), 2)\n",
    "        out = self.out(attr)\n",
    "        encoder_output = F.tanh(out)\n",
    "        return attr, encoder_output\n",
    "    \n",
    "    def get_tag_len(self, tag): \n",
    "        \"\"\"padding 제외한 token 개수\"\"\"\n",
    "        return torch.sum(tag!=self.config.padding_idx, 1).unsqueeze(1).unsqueeze(1).type(torch.float)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing encoder... with single input =====\n",
      "torch.Size([1, 1, 192])\n",
      "torch.Size([1, 1, 1024])\n",
      "\n",
      "===== with multiple inputs =====\n",
      "torch.Size([2, 1, 192])\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Testing encoder... with single input =====\")\n",
    "config = Config()\n",
    "encoder = Encoder(config)\n",
    "rating = torch.tensor([[3]]).type(torch.long)    # idx of rating in tensor\n",
    "category = torch.tensor([[7]]).type(torch.long)  # idx of category in tensor\n",
    "tag = torch.tensor([[1,2,1]]).type(torch.long)    # idxs of tag\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())\n",
    "\n",
    "print(\"\\n===== with multiple inputs =====\")\n",
    "rating = torch.tensor([[3],[4]])\n",
    "category = torch.tensor([[8],[1]])\n",
    "tag = torch.tensor([[1,2,1,2], [0,1,1,0]])    # 이렇게 하려면 padding 되어 있어야해!\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # TODO: if self.config.pretrained = True\n",
    "        self.embedding = nn.Embedding(self.config.output_size, self.config.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.config.hidden_size, self.config.hidden_size, \\\n",
    "                            num_layers=self.config.num_layers, dropout=self.config.dropout, \\\n",
    "                           batch_first=True)\n",
    "        self.out = nn.Linear(self.config.hidden_size, self.config.output_size)\n",
    "        \n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_token: TENSOR of shape (batch_size, 1)\n",
    "            hidden: from last hidden of encoder\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # 가운데 1이니까 unroll방식으로만!\n",
    "        output = self.embedding(input_token)\n",
    "        # LSTM의 hidden은 (hx, cx)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====testing decoder with encoder_output...(batch=2)====\n",
      "\n",
      "input_token.size():  torch.Size([2, 1])\n",
      "hidden[0].size():  torch.Size([2, 2, 512])\n",
      "hidden[1].size():  torch.Size([2, 2, 512])\n",
      "output.size():  torch.Size([2, 1, 21038])\n"
     ]
    }
   ],
   "source": [
    "print(\"====testing decoder with encoder_output...(batch=2)====\\n\")\n",
    "decoder = Decoder(config)\n",
    "# h_0을 초기화시키고, c_0은 냅두기!!!(0으로 하든가 임의값으로 하든가)\n",
    "batch_size = encoder_output.size(0)    # same as len(encoder_output) and \n",
    "h_ = encoder_output.view(config.num_layers, 2, config.hidden_size)    # 2는 batch_size\n",
    "c_ = torch.zeros_like(h_)\n",
    "hidden = (h_, c_)\n",
    "\n",
    "input_token = torch.tensor([[1],[2]])  # shape (batch_size, seq_len)\n",
    "output, hidden = decoder(input_token, hidden)\n",
    "print(\"input_token.size(): \", input_token.size())\n",
    "print(\"hidden[0].size(): \", hidden[0].size())\n",
    "print(\"hidden[1].size(): \", hidden[1].size())\n",
    "print(\"output.size(): \", output.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def splitHidden(encoder_output, config):\n",
    "#     h_0 = encoder_output.view(config.num_layers, encoder_output.size(0), \\\n",
    "#                               config.hidden_size)\n",
    "#     c_0 = torch.zeros_like(h_0) \n",
    "#     return (h_0, c_0)\n",
    "\n",
    "# def train(encoder, decoder, data, loss_fn, optimizer, batch_size, num_steps, print_every, verbose=False):\n",
    "#     encoder.train()\n",
    "#     decoder.train()\n",
    "\n",
    "#     for t in tqdm_notebook(range(num_steps)):\n",
    "#         optimizer.zero_grad()\n",
    "#         rating_tensor, category_tensor, tag_tensor, target_tensor = data_iterator(data, batch_size)\n",
    "#         target_length = target_tensor.size(-1)\n",
    "        \n",
    "#         attr, encoder_output = encoder(rating_tensor, category_tensor, tag_tensor)\n",
    "#         decoder_hidden = splitHidden(encoder_output, encoder.config)        \n",
    "        \n",
    "#         decoder_input = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "#         loss = 0\n",
    "#         for idx in range(target_length): \n",
    "#             decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)            \n",
    "#             topv, topi = decoder_output.topk(1)\n",
    "#             decoder_input = topi.detach().view(batch_size, 1)\n",
    "#             loss += loss_fn(decoder_output.squeeze(), target_tensor[:,idx])\n",
    "#         num_actual_token = torch.sum(target_tensor != encoder.config.padding_idx).item()\n",
    "#         loss /= num_actual_token\n",
    "        \n",
    "#         if t % print_every == 0:\n",
    "#             print(\"loss at %d step: %f\" % (t, loss))\n",
    "#         loss.backward()\n",
    "# #         nn.utils.clip_grad_norm_(encoder.parameters(), 0.25)\n",
    "# #         nn.utils.clip_grad_norm_(encoder.parameters(), 0.25)\n",
    "#         optimizer.step()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def splitHidden(encoder_output, config):\n",
    "    h_0 = encoder_output.view(config.num_layers, encoder_output.size(0), \\\n",
    "                              config.hidden_size)\n",
    "    c_0 = torch.zeros_like(h_0) \n",
    "    return (h_0, c_0)\n",
    "\n",
    "def train(encoder, decoder, data, loss_fn, optimizer, batch_size, num_steps, print_every, verbose=False):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        rating_tensor, category_tensor, tag_tensor, target_tensor = data_iterator(data, batch_size)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        \n",
    "        attr, encoder_output = encoder(rating_tensor, category_tensor, tag_tensor)\n",
    "        decoder_hidden = splitHidden(encoder_output, encoder.config)        \n",
    "        \n",
    "        decoder_input = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        decoder_outputs = []\n",
    "        for idx in range(target_length): \n",
    "            \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach().view(batch_size, 1)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "        # 이 아래 두 개 shape 조정은 loss function 요구 사항 맞추기 위함!\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 1).view(batch_size*target_length, -1)\n",
    "        target_tensor = target_tensor.view(-1)\n",
    "        loss = loss_fn(decoder_outputs, target_tensor) \n",
    "        num_actual_token = torch.sum(target_tensor != encoder.config.padding_idx).item()\n",
    "        loss /= num_actual_token\n",
    "        \n",
    "        if t % print_every == 0:\n",
    "            print(\"loss at %d step: %f\" % (t, loss))\n",
    "\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae57ff39565240238031bf4651f7e22c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss at 0 step: 9.952996\n",
      "loss at 1 step: 9.942863\n",
      "loss at 2 step: 9.853773\n",
      "loss at 3 step: 9.812474\n",
      "loss at 4 step: 9.472528\n",
      "loss at 5 step: 8.640425\n",
      "loss at 6 step: 9.127494\n",
      "loss at 7 step: 6.658671\n",
      "loss at 8 step: 7.340724\n",
      "loss at 9 step: 7.641273\n",
      "loss at 10 step: 9.708379\n",
      "loss at 11 step: 8.097731\n",
      "loss at 12 step: 8.834807\n",
      "loss at 13 step: 8.271302\n",
      "loss at 14 step: 8.786379\n",
      "loss at 15 step: 8.618076\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-38145e504847>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#train(encoder, decoder, data, loss_fn, optimizer, config.batch_size, config.num_steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-7270b32a390e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, data, loss_fn, optimizer, batch_size, num_steps, print_every, verbose)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     87\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     88\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "encoder = Encoder(config)\n",
    "decoder = Decoder(config)\n",
    "# data is defined on top\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "loss_fn = nn.NLLLoss(size_average=False, ignore_index=config.padding_idx)\n",
    "optimizer = optim.Adam(params, lr=0.002)\n",
    "#train(encoder, decoder, data, loss_fn, optimizer, config.batch_size, config.num_steps)\n",
    "train(encoder, decoder, data, loss_fn, optimizer, 2, 1000, 1, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2Seq(nn.Module):\n",
    "    def __init__(self, config, criterion):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, rating, category, tag, target_tensor):\n",
    "        # 함수 호출시 *[rating, category, tag]하기!\n",
    "\n",
    "        batch_size = target_tensor.size(0)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        attr, encoder_output = self.encoder(rating,category,tag)\n",
    "        batch_size = encoder_output.size(0)\n",
    "        decoder_hidden = self.splitHidden(encoder_output)\n",
    "        input_token = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        \n",
    "        decoder_outputs = []\n",
    "        for idx in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(input_token, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input_token = topi.detach().view(batch_size, 1)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "        # 이 아래 두 개 shape 조정은 loss function 요구 사항 맞추기 위함!\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 1).view(batch_size*target_length, -1)\n",
    "        target_tensor = target_tensor.view(-1)\n",
    "        loss = self.criterion(decoder_outputs, target_tensor) #/ batch_size\n",
    "                        # batch_size로 나눠주기\n",
    "#         print(\"decoder_outputs: \", decoder_outputs)\n",
    "#         print(\"target_tensor: \", target_tensor)\n",
    "        return loss\n",
    "    \n",
    "    def splitHidden(self, encoder_output):\n",
    "        h_0 = encoder_output.view(self.config.num_layers, encoder_output.size(0), \\\n",
    "                                  self.config.hidden_size)\n",
    "        c_0 = torch.zeros_like(h_0) \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def inference(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== testing Attr2Seq model... ====\n",
      "tensor(10.0135)\n",
      "\n",
      "==== baching(size 2) ====\n",
      "tensor(10.0085)\n"
     ]
    }
   ],
   "source": [
    "print(\"==== testing Attr2Seq model... ====\")\n",
    "criterion = nn.NLLLoss()     # TODO: padding loss빼주기ㅠㅠ\n",
    "model = Attr2Seq(config, criterion)\n",
    "input_list = [torch.tensor([[2]]).type(torch.long), torch.tensor([[7]]).type(torch.long),\n",
    "              torch.tensor([[0,2,1]])]\n",
    "target_tensor = torch.tensor([[4]])\n",
    "loss = model(*input_list, target_tensor)\n",
    "print(loss)\n",
    "\n",
    "print(\"\\n==== baching(size 2) ====\")\n",
    "input_list = [torch.tensor([[2],[1]]).type(torch.long), torch.tensor([[7],[5]]).type(torch.long),\n",
    "              torch.tensor([[0,2,1],[2,0,1]])]\n",
    "target_tensor = torch.tensor([[9,2,1,3,5], [3,2,3,2,1]])\n",
    "loss = model(*input_list, target_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 전에 Train / Dev 나누기\n",
    "# TODO: data랑 받아서 정의된 data iterator로 불러오기\n",
    "\n",
    "def train(data, model, optimizer, num_steps):\n",
    "    model.train()\n",
    "\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "#        a = list(model.parameters())[0]\n",
    "        optimizer.zero_grad()\n",
    "        r, c, t, rv = data_iterator(data, batch_size = 2)\n",
    "        loss = model(r, c, t, rv)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()        \n",
    "#         for name, param in model.named_parameters():\n",
    "#             print(name)\n",
    "#             print(param.grad)\n",
    "#        b = list(model.parameters())[0]\n",
    "#        print(loss)\n",
    "    return loss.item()# / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081668a859a54593ae6165a8fc927079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "model = Attr2Seq(config, criterion)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\n",
    "temp_loss = train(Encoded, model, optimizer, 2)\n",
    "print(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2Seq(nn.Module):\n",
    "    def __init__(self, config, criterion):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, rating, category, tag, target_tensor):\n",
    "\n",
    "        batch_size = target_tensor.size(0)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        attr, encoder_output = self.encoder(rating,category,tag)\n",
    "        batch_size = encoder_output.size(0)\n",
    "        decoder_hidden = self.splitHidden(encoder_output)\n",
    "        input_token = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        \n",
    "        loss = 0\n",
    "        for idx in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(input_token, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input_token = topi.detach().view(batch_size, 1)\n",
    "            loss += self.criterion(decoder_output.squeeze(), target_tensor[:,idx].view(-1))\n",
    "\n",
    "        loss = loss / batch_size\n",
    "        return loss\n",
    "    \n",
    "    def splitHidden(self, encoder_output):\n",
    "        h_0 = encoder_output.view(self.config.num_layers, encoder_output.size(0), \\\n",
    "                                  self.config.hidden_size)\n",
    "        c_0 = torch.zeros_like(h_0) \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def inference(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 전에 Train / Dev 나누기\n",
    "# TODO: data랑 받아서 정의된 data iterator로 불러오기\n",
    "\n",
    "def train(data, model, criterion, optimizer, num_steps):\n",
    "    model.train()\n",
    "    loss_fn = criterion\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "        a = list(model.parameters())[0]\n",
    "        optimizer.zero_grad()\n",
    "        r, c, t, rv = data_iterator(data, batch_size = 2)\n",
    "            \n",
    "        loss = model(r, c, t, rv)\n",
    "#        decoder_output = model(r, c, t, rv)\n",
    "#        rv = rv.view(-1)\n",
    "#        loss = loss_fn(decoder_output, rv)\n",
    "        loss.backward()\n",
    "#        print(loss.grad_fn)\n",
    "        print(loss)\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()        \n",
    "        for name, param in model.named_parameters():\n",
    "            print(name)\n",
    "            print(param.grad)\n",
    "        b = list(model.parameters())[0]\n",
    "        \n",
    "        print(loss)\n",
    "    return loss.item()# / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508628787c043a888c6e378216ae8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/tqdm/_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(239.8178)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-0.1394, -0.4734,  0.0901, -0.2424, -0.0275, -0.2197,  0.5335,\n",
      "         -0.1829, -0.4313,  0.5435,  0.6294,  0.0810,  0.1318,  0.9710,\n",
      "          0.2163,  0.3097,  0.1586,  0.3123, -1.0216, -0.0940,  0.0342,\n",
      "         -0.6293,  0.1568,  0.3926,  0.5302,  0.0847, -0.0176,  0.0282,\n",
      "          0.2373, -0.5901, -0.1565, -0.1280, -0.2264,  0.3513, -0.2850,\n",
      "          0.4641, -0.4173,  0.2349, -0.0013,  0.4252,  0.3957, -0.0341,\n",
      "          0.0350,  0.4284, -0.2735,  0.2149,  0.2342, -0.4443,  0.1432,\n",
      "         -0.4402, -0.3124, -0.2952, -0.2387,  0.3001,  0.0590, -0.1819,\n",
      "         -0.1179, -0.0944,  0.2338,  0.3593, -0.2178, -0.0130,  0.3182,\n",
      "         -0.2979],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-1.3002, -1.3762,  0.6344,  0.2776, -1.3070, -1.4853, -0.5950,\n",
      "          0.2295, -0.2682,  1.6754, -1.7311, -1.4065,  0.7270,  0.4997,\n",
      "         -1.3453,  1.4607,  1.5160, -0.5727,  1.4798, -0.6808,  3.9223,\n",
      "          0.6346, -1.8802,  1.7132,  2.5296, -0.6179, -1.4657, -0.1866,\n",
      "         -0.0342,  0.9944,  3.4486,  0.4888, -0.1171,  0.2087, -0.5427,\n",
      "          0.2740, -0.9423, -0.9543, -1.2520,  2.6885, -0.6509, -0.1169,\n",
      "          0.2064, -1.6798, -1.3745, -0.3588, -1.2679,  0.4022,  0.5690,\n",
      "          3.2051, -1.6332, -2.4246,  0.0360, -0.0170,  1.2612, -0.5281,\n",
      "         -1.1087,  1.5464, -3.2279,  2.1084, -0.7763,  0.5661, -1.3028,\n",
      "         -0.9439],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4094, -0.2905,  0.0694,  ..., -0.0651,  0.0056,  0.1072],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[ 3.0455e-05,  6.4547e-06,  3.9626e-05,  ...,  1.7587e-05,\n",
      "         -2.8142e-06,  9.0993e-05],\n",
      "        [-8.5778e-05, -1.6131e-05, -1.1409e-04,  ..., -4.9618e-05,\n",
      "          6.8098e-06, -2.5871e-04],\n",
      "        [ 1.2599e-05,  8.8920e-06,  8.8665e-06,  ...,  7.0212e-06,\n",
      "         -4.5551e-06,  3.0283e-05],\n",
      "        ...,\n",
      "        [-7.0538e-05, -1.7491e-05, -8.8704e-05,  ..., -4.0630e-05,\n",
      "          7.9032e-06, -2.0774e-04],\n",
      "        [-8.3883e-05, -1.7260e-05, -1.0977e-04,  ..., -4.8462e-05,\n",
      "          7.4685e-06, -2.5124e-04],\n",
      "        [-4.8691e-05, -1.4112e-05, -5.8766e-05,  ..., -2.7963e-05,\n",
      "          6.5661e-06, -1.4099e-04]])\n",
      "encoder.out.bias\n",
      "tensor([-5.4033e-04,  1.4847e-03, -3.3619e-04,  ...,  1.2975e-03,\n",
      "         1.4788e-03,  9.3254e-04])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(239.8178)\n",
      "tensor(184.9530)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[-1.3474, -1.7485, -0.5939,  0.9072,  0.2683, -1.3788,  1.2970,\n",
      "          1.5173, -0.4313,  0.4661, -0.2176,  0.3852,  0.7642, -0.5767,\n",
      "         -0.6824, -1.1337,  1.1761, -0.8963, -1.1321, -1.6096,  3.0573,\n",
      "          1.9313, -1.0760,  2.7680,  2.6189,  0.3410, -1.9148, -1.9158,\n",
      "          1.7822, -0.2703,  1.7105,  1.4060, -0.1042,  1.3080, -2.1033,\n",
      "          0.2506,  1.2573, -0.0276,  0.1767,  3.0383, -1.7698,  1.8860,\n",
      "         -0.7465, -0.4451, -3.5081,  0.8240, -2.0428,  0.0684,  2.8931,\n",
      "          2.6074, -0.4980, -0.6003, -0.1587,  2.3374,  1.8680, -0.9444,\n",
      "         -4.2063,  0.5632, -1.5759,  2.5300,  1.2959,  1.8044, -2.0168,\n",
      "          0.1234],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-04 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[-3.2210e-06, -1.7455e-05,  8.6184e-06,  ..., -1.9152e-06,\n",
      "         -1.6223e-06, -9.1215e-07],\n",
      "        [ 2.2034e-05,  1.1940e-04, -5.8955e-05,  ...,  1.1937e-05,\n",
      "          1.4321e-05,  3.3050e-06],\n",
      "        [-6.0278e-06, -3.2665e-05,  1.6129e-05,  ..., -5.4628e-06,\n",
      "          2.1651e-06, -6.4425e-06],\n",
      "        ...,\n",
      "        [ 1.5155e-05,  8.2124e-05, -4.0550e-05,  ...,  7.2880e-06,\n",
      "          1.2403e-05, -5.1685e-08],\n",
      "        [ 2.7186e-05,  1.4732e-04, -7.2742e-05,  ...,  1.8101e-05,\n",
      "          8.3336e-06,  1.2578e-05],\n",
      "        [ 3.6138e-05,  1.9583e-04, -9.6695e-05,  ...,  2.4279e-05,\n",
      "          1.0475e-05,  1.7269e-05]])\n",
      "encoder.out.bias\n",
      "tensor(1.00000e-03 *\n",
      "       [-0.2292,  1.5676, -0.4288,  ...,  1.0782,  1.9342,  2.5711])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(184.9530)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.0357)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-1.7598, -0.9653, -1.0876, -0.9319, -0.5623, -1.0171,  1.0912,\n",
      "          2.0564, -2.1061,  1.1830,  0.0069, -0.3403,  0.8550,  0.0446,\n",
      "         -2.6909,  0.3595, -0.0877, -1.1301, -1.1464, -1.3937,  3.3239,\n",
      "          0.6338, -2.7224,  1.8870,  1.4685,  0.1902, -1.5925, -1.9791,\n",
      "         -0.7989,  0.2858,  3.5813,  0.2917, -1.2962, -1.0036, -2.0289,\n",
      "          0.5360,  0.5821, -1.4962,  0.2813,  3.5992, -0.0155,  1.5918,\n",
      "          0.3996, -0.1368, -3.4686,  1.0226, -1.9875, -0.5433, -0.2208,\n",
      "          0.5349, -0.5199, -0.8176,  1.5497,  1.5824,  2.7518,  0.1363,\n",
      "         -2.8182,  2.8445, -1.7713,  1.0375,  1.6412,  0.9582, -1.5797,\n",
      "         -1.0454],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3429,  0.2823,  0.5932,  ...,  0.3328, -0.5151,  0.1138],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[ 4.8431e-05,  1.1360e-05,  6.2604e-05,  ...,  1.0180e-05,\n",
      "          1.3778e-07,  2.2405e-05],\n",
      "        [-5.3447e-05, -1.2537e-05, -6.9087e-05,  ..., -1.0665e-05,\n",
      "          6.3826e-08, -2.4007e-05],\n",
      "        [ 3.7661e-05,  8.8339e-06,  4.8682e-05,  ...,  8.3262e-06,\n",
      "          2.6250e-07,  1.7939e-05],\n",
      "        ...,\n",
      "        [-2.4385e-05, -5.7198e-06, -3.1521e-05,  ..., -5.4601e-06,\n",
      "         -1.9613e-07, -1.1702e-05],\n",
      "        [-5.0419e-05, -1.1827e-05, -6.5174e-05,  ..., -9.6141e-06,\n",
      "          2.2965e-07, -2.2084e-05],\n",
      "        [-1.3981e-04, -3.2794e-05, -1.8072e-04,  ..., -2.4601e-05,\n",
      "          1.4168e-06, -5.8645e-05]])\n",
      "encoder.out.bias\n",
      "tensor([-9.1859e-04,  1.0137e-03, -7.1432e-04,  ...,  4.6251e-04,\n",
      "         9.5630e-04,  2.6517e-03])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(55.0357)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.]])\n",
      "encoder.out.bias\n",
      "tensor([nan., nan., nan.,  ..., nan., nan., nan.])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(nan.)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.]])\n",
      "encoder.out.bias\n",
      "tensor([nan., nan., nan.,  ..., nan., nan., nan.])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(nan.)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-72cdc8c4a3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtemp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-28dae72990bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, criterion, optimizer, num_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', dtype='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_number_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSCALE_FORMAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_number_format\u001b[0;34m(tensor, min_sz)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# TODO: use fmod?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mint_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "model = Attr2Seq(config, criterion)\n",
    "params = list(model.encoder.parameters()) + list(model.decoder.parameters())\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "optimizer = optim.SGD(params, lr=1)\n",
    "\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\n",
    "temp_loss = train(Encoded, model, criterion, optimizer, 10)\n",
    "print(temp_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hwijeen_3.6]",
   "language": "python",
   "name": "conda-env-hwijeen_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
