{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "1. Batching!   \n",
    "    -Encoder.Forward의 input 모양 어떻게 되지? / .view 인자 확인!\n",
    "2. Attention  \n",
    "3. Teacher Forcing  \n",
    "4. Parameter(things to be updated) 등록 잘 됐나 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "1. Decoder가 2 layer일때, initial hidden?  \n",
    "    - https://discuss.pytorch.org/t/understanding-output-of-lstm/12320/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    dropout = 0.2\n",
    "    \n",
    "    # Encoder\n",
    "    rating_size = 5\n",
    "    category_size = 10\n",
    "    tag_size = 3\n",
    "    pretrained = False \n",
    "    # embedding_size = 300    # needed when not using pretrained vector\n",
    "    attribute_size = 64\n",
    "    hidden_size = 512 # fixed-vector size \n",
    "    # word-embedding = hidden_size\n",
    "    \n",
    "    # Decoder\n",
    "    num_layers = 2\n",
    "    output_size = 10\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # Embedding instead of Linear for efficient indexing\n",
    "        self.emb_rating = nn.Embedding(self.config.rating_size, self.config.attribute_size)   \n",
    "        self.emb_category = nn.Embedding(self.config.category_size, self.config.attribute_size)\n",
    "        self.emb_tag = nn.Embedding(self.config.tag_size, self.config.attribute_size)\n",
    "        self.out = nn.Linear(self.config.attribute_size * 3, self.config.hidden_size*self.config.num_layers)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def forward(self, rating, category, tag):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            rating: TENSOR of shape (batch_size, 1)\n",
    "            category: TENSOR of shape (batch_size, 1)\n",
    "            tag : 1) TENSOR of shape (batch_size, tag_MAXLEN)\n",
    "        Returns:\n",
    "            concatenated attr for attention, encoder_output\n",
    "        \"\"\"\n",
    "        # TODO: check if len(rating), len(category), len(tag) matches\n",
    "        attr_rating = self.emb_rating(rating).view(rating.size(0),1,-1)    \n",
    "        attr_category = self.emb_category(category).view(category.size(0),1,-1)\n",
    "        attr_tag = torch.sum(self.emb_tag(tag), 1) / len(tag)    # embedding 평균\n",
    "        attr_tag = attr_tag.view(tag.size(0),1,-1)\n",
    "        \n",
    "        attr = torch.cat((attr_rating, attr_category, attr_tag), 2)    # specify dim?\n",
    "        out = self.out(attr)\n",
    "        encoder_output = F.tanh(out)\n",
    "        return attr, encoder_output\n",
    "    \n",
    "    def init_hidden(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing encoder... with single input =====\n",
      "torch.Size([1, 1, 192])\n",
      "torch.Size([1, 1, 1024])\n",
      "\n",
      "===== with multiple inputs =====\n",
      "torch.Size([2, 1, 192])\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Testing encoder... with single input =====\")\n",
    "config = Config()\n",
    "encoder = Encoder(config)\n",
    "rating = torch.tensor([3]).type(torch.long)    # idx of rating in tensor\n",
    "category = torch.tensor([7]).type(torch.long)  # idx of category in tensor\n",
    "tag = torch.tensor([[1,2,1]]).type(torch.long)    # CBOW of one-hot\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())\n",
    "\n",
    "print(\"\\n===== with multiple inputs =====\")\n",
    "rating = torch.tensor([3,4])\n",
    "category = torch.tensor([8,1])\n",
    "tag = torch.tensor([[1,2,1], [0,1,1]])\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # TODO: if self.config.pretrained = True\n",
    "        self.embedding = nn.Embedding(self.config.output_size, self.config.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.config.hidden_size, self.config.hidden_size, \\\n",
    "                            num_layers=self.config.num_layers, dropout=self.config.dropout)\n",
    "        self.out = nn.Linear(self.config.hidden_size, self.config.output_size)\n",
    "        \n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_token: TENSOR of shape (1,1,1)\n",
    "            hidden: from last hidden of encoder\n",
    "        Returns:\n",
    "            concatenated attr for attention, encoder_output\n",
    "        \"\"\"\n",
    "        output = self.embedding(input_token).view(len(input_token), 1, -1)    # ?\n",
    "        # LSTM의 hidden은 (hx, cx)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing decoder with encoder_output...\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 2: size '[2 x 1 x 512]' is invalid for input with 2048 elements at /opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/TH/THStorage.c:41",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e7920d16bfa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# h_ = torch.split(encoder_output,[config.hidden_size, config.hidden_size], 2)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# h_ = torch.cat(h_, dim=0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mh_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mc_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 2: size '[2 x 1 x 512]' is invalid for input with 2048 elements at /opt/conda/conda-bld/pytorch_1525812548180/work/aten/src/TH/THStorage.c:41"
     ]
    }
   ],
   "source": [
    "print(\"testing decoder with encoder_output...\\n\")\n",
    "decoder = Decoder(config)\n",
    "# h_ = torch.split(encoder_output,[config.hidden_size, config.hidden_size], 2)\n",
    "# h_ = torch.cat(h_, dim=0)\n",
    "h_ = encoder_output.view(config.num_layers, 1, config.hidden_size)\n",
    "c_ = encoder_output.view(config.num_layers, 1, config.hidden_size)\n",
    "hidden = h_, c_\n",
    "\n",
    "input_token = torch.tensor([0,0,0])\n",
    "output, hidden = decoder(input_token, hidden)\n",
    "print(\"input_token.size(): \", input_token.size())\n",
    "print(\"hidden[0].size(): \", hidden[0].size())\n",
    "print(\"hidden[1].size(): \", hidden[1].size())\n",
    "print(\"output.size(): \", output.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2Seq(nn.Module):\n",
    "    def __init__(self, config, criterion):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, rating, category, tag, target_tensor):\n",
    "        # 함수 호출시 *[rating, category, tag]하기!\n",
    "        # target_tensor도 1차원!!!!\n",
    "        target_length = target_tensor.size(0)    \n",
    "        attr, encoder_output = self.encoder(rating,category,tag)\n",
    "        \n",
    "        hidden = self.splitHidden(encoder_output)\n",
    "        input_token = torch.zeros((1,1,1)).type(torch.long)    # SOS token\n",
    "        \n",
    "        decoder_outputs = []\n",
    "        for idx in range(target_length):\n",
    "            decoder_output, decoder_hidden = decoder(input_token, hidden)\n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input_token = topi.detach()            \n",
    "            decoder_outputs.append(decoder_output.squeeze())\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 0).view(target_length, -1)\n",
    "        loss = self.criterion(decoder_outputs, target_tensor)\n",
    "        return loss\n",
    "    \n",
    "    def splitHidden(self, encoder_output):\n",
    "        \"\"\"\n",
    "        Encoder의 ouput인 fixed size vector를 Decoder의 hidden으로 쪼개기\n",
    "        \"\"\"\n",
    "        return encoder_output.view(self.config.num_layers, 1, self.config.hidden_size), \\\n",
    "                encoder_output.view(self.config.num_layers, 1, self.config.hidden_size)    \n",
    "    \n",
    "    def inference(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2990)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "model = Attr2Seq(config, criterion)\n",
    "input_list = [torch.tensor([2]).type(torch.long), torch.tensor([7]).type(torch.long),\n",
    "              torch.tensor([0,2,1])]\n",
    "#target_tensor = torch.tensor([[3],[3],[4],[6]])\n",
    "target_tensor = torch.tensor([9,2,1,3])\n",
    "loss = model(*input_list, target_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_list, target_tensor, model, optimizer):\n",
    "    \"\"\"\n",
    "    perform a step(update)\n",
    "    \"\"\"\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    target_length = target_tensor.size(0)\n",
    "    \n",
    "    loss = model(*input_list, target_tensor)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item() / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5761695504188538\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.002)\n",
    "avg_loss = train(input_list, target_tensor, model, optimizer)\n",
    "print(avg_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateRandomly():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attr2Seq(\n",
      "  (criterion): NLLLoss()\n",
      "  (encoder): Encoder(\n",
      "    (emb_rating): Embedding(5, 64)\n",
      "    (emb_category): Embedding(10, 64)\n",
      "    (emb_tag): Embedding(3, 64)\n",
      "    (out): Linear(in_features=192, out_features=1024, bias=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(10, 512)\n",
      "    (lstm): LSTM(512, 512, num_layers=2, dropout=0.2)\n",
      "    (out): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json으로 param관리\n",
    "logger 관리\n",
    "\n",
    "1. load data(split sets)\n",
    "    do build_vocab first\n",
    "    Class DataLoader\n",
    "2. instantiate model\n",
    "3. get optimizer\n",
    "4. define loss function\n",
    "5. get into train\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hwijeen_3.6]",
   "language": "python",
   "name": "conda-env-hwijeen_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
