{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "~~1. Batching!~~   \n",
    "    ~~-Encoder.Forward의 input 모양 어떻게 되지? / .view 인자 확인!~~\n",
    "2. Attention  \n",
    "3. Teacher Forcing  \n",
    "~~4. Parameter(things to be updated) 등록 잘 됐나 확인(= 그래프 잘 그린 건지 어케 확인하나?)~~\n",
    "5. Train / Dev 사전에 나누기\n",
    "6. Pretrained word vector을 쓸 수가 있나..?  \n",
    "** SOS, EOS, PAD token 관리 어디서?**   \n",
    "** INTO GPU: x batch, y batch, parameter On GPU**\n",
    "7. model save등등 train 뒷단 얘기  \n",
    "    -torch.save\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE\n",
    "1. Decoder가 2 layer일때, initial hidden?  \n",
    "    - https://discuss.pytorch.org/t/understanding-output-of-lstm/12320/2\n",
    "    - hidden 의 dimension\n",
    "2. embedding 거친 후엔 batch_size x seq_len x embedding_dim 임!\n",
    "    - embedding input은 batch_size x seq_len\n",
    "2. LSTM의 batch_first\n",
    "3. LSTM input의 dimension\n",
    "4. NLLLoss()의 dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from build_vocab import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab for tags ...\n",
      "Building vocab for reviews ...\n",
      "3339 out of 3465 words left, which is 96.36363636363636 %\n",
      "21036 out of 37012 words left, which is 56.83562087971469 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ef5d5a0c5b142ee98086292c7737b85",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=163733), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "meta = build_meta()\n",
    "tagVocab, rvVocab = filter_by_cnt(2, 2, 'data/products.pkl')\n",
    "data = prepareData(meta, tagVocab, rvVocab, 'data/products.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    dropout = 0.2\n",
    "    batch_size = 2\n",
    "    num_steps = 10\n",
    "    \n",
    "    # Encoder\n",
    "    rating_size = len(meta.rating2idx)\n",
    "    category_size = len(meta.lowcat2idx)\n",
    "    tag_size = len(tagVocab.word2idx)    # tag vocab_size\n",
    "    pretrained = False \n",
    "    attribute_size = 64\n",
    "    hidden_size = 512 # fixed-vector size \n",
    "    \n",
    "    # Decoder\n",
    "    num_layers = 2\n",
    "    output_size =  len(rvVocab.word2idx)    # review vocab_size\n",
    "    \n",
    "    # TEMP - 이거 어디서 관리하지? 여긴 아닌뎅\n",
    "    padding_idx = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, config):        \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        self.emb_rating = nn.Embedding(self.config.rating_size, self.config.attribute_size)   \n",
    "        self.emb_category = nn.Embedding(self.config.category_size, self.config.attribute_size)\n",
    "        self.emb_tag = nn.Embedding(self.config.tag_size, self.config.attribute_size,\n",
    "                                   padding_idx=self.config.padding_idx)        \n",
    "        self.out = nn.Linear(self.config.attribute_size * 3, self.config.hidden_size*self.config.num_layers)\n",
    "        self.init_hidden()\n",
    "        \n",
    "    def forward(self, rating, category, tag):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            rating: TENSOR of shape (batch_size, 1)\n",
    "            category: TENSOR of shape (batch_size, 1)\n",
    "            tag : 1) TENSOR of shape (batch_size, tag_MAXLEN)\n",
    "        Returns:\n",
    "            concatenated attr for attention, encoder_output\n",
    "        \"\"\"\n",
    "        \n",
    "        assert len(rating) == len(category) == len(tag)\n",
    "        attr_rating = self.emb_rating(rating)    # view no need!\n",
    "        attr_category = self.emb_category(category)\n",
    "        tag_len = self.get_tag_len(tag)    \n",
    "        attr_tag = torch.sum(self.emb_tag(tag), 1, keepdim=True) / tag_len    # CBOW\n",
    "                # 여기서 이렇게 해도 backprop 맞게 되나??\n",
    "        \n",
    "        attr = torch.cat((attr_rating, attr_category, attr_tag), 2)\n",
    "        out = self.out(attr)\n",
    "        encoder_output = F.tanh(out)\n",
    "        return attr, encoder_output\n",
    "    \n",
    "    def get_tag_len(self, tag): \n",
    "        \"\"\"padding 제외한 token 개수\"\"\"\n",
    "        return torch.sum(tag!=self.config.padding_idx, 1).unsqueeze(1).unsqueeze(1).type(torch.float)\n",
    "        \n",
    "    def init_hidden(self):\n",
    "        for param in self.parameters():\n",
    "            nn.init.uniform_(param, -0.08, 0.08)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Testing encoder... with single input =====\n",
      "torch.Size([1, 1, 192])\n",
      "torch.Size([1, 1, 1024])\n",
      "\n",
      "===== with multiple inputs =====\n",
      "torch.Size([2, 1, 192])\n",
      "torch.Size([2, 1, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(\"===== Testing encoder... with single input =====\")\n",
    "config = Config()\n",
    "encoder = Encoder(config)\n",
    "rating = torch.tensor([[3]]).type(torch.long)    # idx of rating in tensor\n",
    "category = torch.tensor([[7]]).type(torch.long)  # idx of category in tensor\n",
    "tag = torch.tensor([[1,2,1]]).type(torch.long)    # idxs of tag\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())\n",
    "\n",
    "print(\"\\n===== with multiple inputs =====\")\n",
    "rating = torch.tensor([[3],[4]])\n",
    "category = torch.tensor([[8],[1]])\n",
    "tag = torch.tensor([[1,2,1,2], [0,1,1,0]])    # 이렇게 하려면 padding 되어 있어야해!\n",
    "attr, encoder_output = encoder(rating,category,tag)\n",
    "print(attr.size())\n",
    "print(encoder_output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        \n",
    "        # TODO: if self.config.pretrained = True\n",
    "        self.embedding = nn.Embedding(self.config.output_size, self.config.hidden_size)\n",
    "        self.lstm = nn.LSTM(self.config.hidden_size, self.config.hidden_size, \\\n",
    "                            num_layers=self.config.num_layers, dropout=self.config.dropout, \\\n",
    "                           batch_first=True)\n",
    "        self.out = nn.Linear(self.config.hidden_size, self.config.output_size)\n",
    "        \n",
    "    def forward(self, input_token, hidden):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            input_token: TENSOR of shape (batch_size, 1)\n",
    "            hidden: from last hidden of encoder\n",
    "        Returns:\n",
    "        \"\"\"\n",
    "        # 가운데 1이니까 unroll방식으로만!\n",
    "        output = self.embedding(input_token)\n",
    "        # LSTM의 hidden은 (hx, cx)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output)\n",
    "        output = F.log_softmax(output, dim=2)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====testing decoder with encoder_output...(batch=2)====\n",
      "\n",
      "input_token.size():  torch.Size([2, 1])\n",
      "hidden[0].size():  torch.Size([2, 2, 512])\n",
      "hidden[1].size():  torch.Size([2, 2, 512])\n",
      "output.size():  torch.Size([2, 1, 21038])\n"
     ]
    }
   ],
   "source": [
    "print(\"====testing decoder with encoder_output...(batch=2)====\\n\")\n",
    "decoder = Decoder(config)\n",
    "# h_0을 초기화시키고, c_0은 냅두기!!!(0으로 하든가 임의값으로 하든가)\n",
    "batch_size = encoder_output.size(0)    # same as len(encoder_output) and \n",
    "h_ = encoder_output.view(config.num_layers, 2, config.hidden_size)    # 2는 batch_size\n",
    "c_ = torch.zeros_like(h_)\n",
    "hidden = (h_, c_)\n",
    "\n",
    "input_token = torch.tensor([[1],[2]])  # shape (batch_size, seq_len)\n",
    "output, hidden = decoder(input_token, hidden)\n",
    "print(\"input_token.size(): \", input_token.size())\n",
    "print(\"hidden[0].size(): \", hidden[0].size())\n",
    "print(\"hidden[1].size(): \", hidden[1].size())\n",
    "print(\"output.size(): \", output.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitHidden(encoder_output, config):\n",
    "    h_0 = encoder_output.view(config.num_layers, encoder_output.size(0), \\\n",
    "                              config.hidden_size)\n",
    "    c_0 = torch.zeros_like(h_0) \n",
    "    return (h_0, c_0)\n",
    "\n",
    "def train(encoder, decoder, data, loss_fn, optimizer, batch_size, num_steps):\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "        optimizer.zero_grad()\n",
    "        rating_tensor, category_tensor, tag_tensor, target_tensor = data_iterator(data, batch_size)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        \n",
    "        attr, encoder_output = encoder(rating_tensor, category_tensor, tag_tensor)\n",
    "        decoder_hidden = splitHidden(encoder_output, encoder.config)        \n",
    "        \n",
    "        decoder_input = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        loss = 0\n",
    "        #print(\"target_length in this batch: %d\" % target_length)\n",
    "        for idx in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            decoder_input = topi.detach().view(batch_size, 1)\n",
    "            loss += loss_fn(decoder_output.squeeze(), target_tensor[:,idx])\n",
    "        num_actual_token = torch.sum(target_tensor != encoder.config.padding_idx).item()\n",
    "        loss /= num_actual_token\n",
    "        print(\"loss: %f\" % loss)\n",
    "        \n",
    "        loss.backward()\n",
    "#         nn.utils.clip_grad_norm_(encoder.parameters(), 0.25)\n",
    "#         nn.utils.clip_grad_norm_(encoder.parameters(), 0.25)\n",
    "        optimizer.step()     \n",
    "\n",
    "#         for name, param in decoder.named_parameters():\n",
    "#             print(\"decoder \"+name, param.grad)\n",
    "#         for name, param in encoder.named_parameters():\n",
    "#             print(\"encoder \"+name, param.grad)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "227a91abfd704c9c8c3d6725b86a9264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-89:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/tqdm/_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 9.958877\n",
      "loss: 9.913923\n",
      "loss: 9.790391\n",
      "loss: 9.021402\n",
      "loss: 8.166480\n",
      "loss: 7.414064\n",
      "loss: 7.272287\n",
      "loss: 6.905063\n",
      "loss: 6.793014\n",
      "loss: 6.947257\n",
      "loss: 7.099370\n",
      "loss: 6.957481\n",
      "loss: 6.647564\n",
      "loss: 6.983301\n",
      "loss: 7.597042\n",
      "loss: 7.219642\n",
      "loss: 6.935116\n",
      "loss: 7.469126\n",
      "loss: 6.923403\n",
      "loss: 7.166224\n",
      "loss: 6.977667\n",
      "loss: 7.349732\n",
      "loss: 7.071460\n",
      "loss: 6.831165\n",
      "loss: 6.951372\n",
      "loss: 6.689351\n",
      "loss: 6.972092\n",
      "loss: 6.986296\n",
      "loss: 6.891676\n",
      "loss: 6.695094\n",
      "loss: 7.070732\n",
      "loss: 7.055617\n",
      "loss: 7.232532\n",
      "loss: 6.935348\n",
      "loss: 7.107073\n",
      "loss: 6.647736\n",
      "loss: 6.915483\n",
      "loss: 6.622308\n",
      "loss: 7.273326\n",
      "loss: 7.156304\n",
      "loss: 6.969756\n",
      "loss: 6.842958\n",
      "loss: 6.795801\n",
      "loss: 6.873309\n",
      "loss: 6.774458\n",
      "loss: 6.709067\n",
      "loss: 6.811054\n",
      "loss: 6.624053\n",
      "loss: 6.917876\n",
      "loss: 6.663665\n",
      "loss: 6.901025\n",
      "loss: 6.628020\n",
      "loss: 6.941717\n",
      "loss: 6.611979\n",
      "loss: 6.421379\n",
      "loss: 6.946917\n",
      "loss: 7.183492\n",
      "loss: 6.769335\n",
      "loss: 6.534405\n",
      "loss: 6.901868\n",
      "loss: 6.846897\n",
      "loss: 6.745937\n",
      "loss: 6.702459\n",
      "loss: 6.395546\n",
      "loss: 6.509654\n",
      "loss: 6.792053\n",
      "loss: 6.575664\n",
      "loss: 6.095592\n",
      "loss: 6.533376\n",
      "loss: 6.719856\n",
      "loss: 6.100523\n",
      "loss: 6.672230\n",
      "loss: 6.444269\n",
      "loss: 6.678984\n",
      "loss: 6.239803\n",
      "loss: 6.449592\n",
      "loss: 6.603388\n",
      "loss: 6.273193\n",
      "loss: 6.699361\n",
      "loss: 6.627208\n",
      "loss: 6.711838\n",
      "loss: 6.761800\n",
      "loss: 6.777938\n",
      "loss: 6.587149\n",
      "loss: 6.192815\n",
      "loss: 6.367917\n",
      "loss: 6.642003\n",
      "loss: 6.886149\n",
      "loss: 6.594275\n",
      "loss: 6.607779\n",
      "loss: 6.370710\n",
      "loss: 6.348626\n",
      "loss: 6.450278\n",
      "loss: 6.461634\n",
      "loss: 6.396089\n",
      "loss: 6.395975\n",
      "loss: 6.405008\n",
      "loss: 6.499588\n",
      "loss: 6.273057\n",
      "loss: 6.249162\n",
      "loss: 6.661487\n",
      "loss: 6.575126\n",
      "loss: 6.628412\n",
      "loss: 6.650496\n",
      "loss: 6.433982\n",
      "loss: 6.369385\n",
      "loss: 6.560374\n",
      "loss: 5.937748\n",
      "loss: 6.034252\n",
      "loss: 6.889880\n",
      "loss: 6.218276\n",
      "loss: 6.406265\n",
      "loss: 6.834179\n",
      "loss: 6.556989\n",
      "loss: 6.588798\n",
      "loss: 6.514919\n",
      "loss: 6.815486\n",
      "loss: 6.226148\n",
      "loss: 6.046052\n",
      "loss: 6.500687\n",
      "loss: 6.617328\n",
      "loss: 6.412505\n",
      "loss: 6.530919\n",
      "loss: 6.234309\n",
      "loss: 6.784245\n",
      "loss: 6.251556\n",
      "loss: 6.525755\n",
      "loss: 6.587905\n",
      "loss: 6.488825\n",
      "loss: 6.526651\n",
      "loss: 6.733958\n",
      "loss: 6.762667\n",
      "loss: 6.394058\n",
      "loss: 6.743606\n",
      "loss: 6.616695\n",
      "loss: 6.408042\n",
      "loss: 6.095294\n",
      "loss: 6.931630\n",
      "loss: 6.426320\n",
      "loss: 6.783735\n",
      "loss: 6.305759\n",
      "loss: 6.195021\n",
      "loss: 6.409865\n",
      "loss: 6.490997\n",
      "loss: 6.508165\n",
      "loss: 6.635766\n",
      "loss: 6.418032\n",
      "loss: 6.601823\n",
      "loss: 6.457253\n",
      "loss: 6.240797\n",
      "loss: 6.614926\n",
      "loss: 6.574831\n",
      "loss: 6.469766\n",
      "loss: 6.422225\n",
      "loss: 6.506566\n",
      "loss: 6.005730\n",
      "loss: 6.234078\n",
      "loss: 6.581157\n",
      "loss: 6.375841\n",
      "loss: 6.245408\n",
      "loss: 6.567179\n",
      "loss: 6.616442\n",
      "loss: 6.360007\n",
      "loss: 6.336123\n",
      "loss: 6.387328\n",
      "loss: 6.521362\n",
      "loss: 6.627606\n",
      "loss: 6.463840\n",
      "loss: 6.287918\n",
      "loss: 6.637542\n",
      "loss: 6.459317\n",
      "loss: 6.539179\n",
      "loss: 6.276204\n",
      "loss: 6.331505\n",
      "loss: 6.487499\n",
      "loss: 6.194791\n",
      "loss: 6.171942\n",
      "loss: 6.027164\n",
      "loss: 6.330317\n",
      "loss: 6.322237\n",
      "loss: 6.375066\n",
      "loss: 6.467302\n",
      "loss: 6.516351\n",
      "loss: 6.507067\n",
      "loss: 6.004903\n",
      "loss: 6.528434\n",
      "loss: 6.454386\n",
      "loss: 6.783876\n",
      "loss: 6.481052\n",
      "loss: 6.864008\n",
      "loss: 6.414047\n",
      "loss: 6.279937\n",
      "loss: 6.314623\n",
      "loss: 6.570654\n",
      "loss: 6.598707\n",
      "loss: 6.386215\n",
      "loss: 6.036497\n",
      "loss: 6.679316\n",
      "loss: 6.562019\n",
      "loss: 6.558587\n",
      "loss: 6.392913\n",
      "loss: 6.615527\n",
      "loss: 6.835545\n",
      "loss: 6.064411\n",
      "loss: 6.726683\n",
      "loss: 6.640832\n",
      "loss: 6.577850\n",
      "loss: 6.266883\n",
      "loss: 5.947909\n",
      "loss: 6.374513\n",
      "loss: 6.371359\n",
      "loss: 6.060575\n",
      "loss: 6.369465\n",
      "loss: 6.254645\n",
      "loss: 6.538818\n",
      "loss: 6.474766\n",
      "loss: 6.627906\n",
      "loss: 6.345294\n",
      "loss: 6.294530\n",
      "loss: 6.161851\n",
      "loss: 6.566098\n",
      "loss: 6.367815\n",
      "loss: 6.458519\n",
      "loss: 6.623164\n",
      "loss: 6.374041\n",
      "loss: 6.295840\n",
      "loss: 6.317266\n",
      "loss: 6.179888\n",
      "loss: 6.437034\n",
      "loss: 6.094176\n",
      "loss: 6.377846\n",
      "loss: 6.234194\n",
      "loss: 6.630819\n",
      "loss: 6.162921\n",
      "loss: 6.556508\n",
      "loss: 6.666339\n",
      "loss: 6.478901\n",
      "loss: 6.511614\n",
      "loss: 6.400779\n",
      "loss: 6.333270\n",
      "loss: 6.320967\n",
      "loss: 6.421435\n",
      "loss: 6.313347\n",
      "loss: 6.355790\n",
      "loss: 6.001917\n",
      "loss: 6.321878\n",
      "loss: 6.557039\n",
      "loss: 6.202460\n",
      "loss: 6.545584\n",
      "loss: 6.140838\n",
      "loss: 6.288444\n",
      "loss: 6.514349\n",
      "loss: 6.256045\n",
      "loss: 6.195293\n",
      "loss: 6.414307\n",
      "loss: 6.661530\n",
      "loss: 6.208106\n",
      "loss: 6.470711\n",
      "loss: 6.331553\n",
      "loss: 6.594392\n",
      "loss: 6.487841\n",
      "loss: 6.398636\n",
      "loss: 6.376399\n",
      "loss: 6.613702\n",
      "loss: 6.482385\n",
      "loss: 6.365695\n",
      "loss: 6.346635\n",
      "loss: 6.374189\n",
      "loss: 6.371386\n",
      "loss: 6.308877\n",
      "loss: 6.567604\n",
      "loss: 6.426903\n",
      "loss: 6.256588\n",
      "loss: 6.104170\n",
      "loss: 6.361360\n",
      "loss: 6.140561\n",
      "loss: 6.499836\n",
      "loss: 6.293862\n",
      "loss: 6.246264\n",
      "loss: 6.293097\n",
      "loss: 6.277049\n",
      "loss: 6.253802\n",
      "loss: 6.401683\n",
      "loss: 6.503808\n",
      "loss: 6.359226\n",
      "loss: 6.258355\n",
      "loss: 6.415103\n",
      "loss: 6.396143\n",
      "loss: 6.068208\n",
      "loss: 6.277138\n",
      "loss: 6.462260\n",
      "loss: 6.390794\n",
      "loss: 6.543159\n",
      "loss: 6.549975\n",
      "loss: 6.316522\n",
      "loss: 6.055933\n",
      "loss: 6.406234\n",
      "loss: 5.906612\n",
      "loss: 6.355241\n",
      "loss: 6.453935\n",
      "loss: 6.450440\n",
      "loss: 6.275987\n",
      "loss: 6.166102\n",
      "loss: 6.439140\n",
      "loss: 6.583981\n",
      "loss: 6.151057\n",
      "loss: 6.535146\n",
      "loss: 6.186818\n",
      "loss: 6.410286\n",
      "loss: 6.390404\n",
      "loss: 6.360920\n",
      "loss: 5.984313\n",
      "loss: 6.659641\n",
      "loss: 6.643990\n",
      "loss: 6.561609\n",
      "loss: 6.048371\n",
      "loss: 6.490135\n",
      "loss: 5.769192\n",
      "loss: 6.156249\n",
      "loss: 6.193823\n",
      "loss: 6.242697\n",
      "loss: 6.232888\n",
      "loss: 6.497726\n",
      "loss: 5.943778\n",
      "loss: 6.312726\n",
      "loss: 6.017712\n",
      "loss: 6.385528\n",
      "loss: 6.611999\n",
      "loss: 6.235842\n",
      "loss: 6.162721\n",
      "loss: 6.316309\n",
      "loss: 6.703851\n",
      "loss: 6.507161\n",
      "loss: 6.599647\n",
      "loss: 6.132083\n",
      "loss: 6.465994\n",
      "loss: 6.325382\n",
      "loss: 6.656862\n",
      "loss: 6.704685\n",
      "loss: 6.574864\n",
      "loss: 6.372683\n",
      "loss: 6.216060\n",
      "loss: 6.418823\n",
      "loss: 6.319073\n",
      "loss: 6.647699\n",
      "loss: 6.189089\n",
      "loss: 6.164412\n",
      "loss: 6.351678\n",
      "loss: 6.509264\n",
      "loss: 6.494089\n",
      "loss: 6.347143\n",
      "loss: 6.522900\n",
      "loss: 6.326866\n",
      "loss: 6.217718\n",
      "loss: 6.265894\n",
      "loss: 6.481225\n",
      "loss: 6.269399\n",
      "loss: 6.176763\n",
      "loss: 6.429749\n",
      "loss: 6.065885\n",
      "loss: 6.548784\n",
      "loss: 6.484430\n",
      "loss: 5.984771\n",
      "loss: 6.545712\n",
      "loss: 6.060101\n",
      "loss: 6.638914\n",
      "loss: 6.268783\n",
      "loss: 6.653167\n",
      "loss: 6.188004\n",
      "loss: 5.993086\n",
      "loss: 6.108338\n",
      "loss: 6.215850\n",
      "loss: 6.393799\n",
      "loss: 6.565958\n",
      "loss: 6.353053\n",
      "loss: 6.194729\n",
      "loss: 6.127534\n",
      "loss: 6.044471\n",
      "loss: 6.343383\n",
      "loss: 6.340060\n",
      "loss: 6.340386\n",
      "loss: 6.452877\n",
      "loss: 6.426050\n",
      "loss: 6.182388\n",
      "loss: 6.478857\n",
      "loss: 6.243112\n",
      "loss: 6.240265\n",
      "loss: 6.400419\n",
      "loss: 6.367960\n",
      "loss: 6.350524\n",
      "loss: 6.437999\n",
      "loss: 6.196241\n",
      "loss: 6.504053\n",
      "loss: 6.139569\n",
      "loss: 6.179427\n",
      "loss: 6.228171\n",
      "loss: 6.224854\n",
      "loss: 5.941026\n",
      "loss: 6.500973\n",
      "loss: 6.341305\n",
      "loss: 7.221759\n",
      "loss: 6.085801\n",
      "loss: 6.155312\n",
      "loss: 6.168367\n",
      "loss: 6.258965\n",
      "loss: 6.098894\n",
      "loss: 6.114757\n",
      "loss: 6.097032\n",
      "loss: 6.642340\n",
      "loss: 6.316851\n",
      "loss: 6.461619\n",
      "loss: 6.280578\n",
      "loss: 6.307881\n",
      "loss: 6.673970\n",
      "loss: 6.112612\n",
      "loss: 6.335855\n",
      "loss: 6.302128\n",
      "loss: 6.450671\n",
      "loss: 6.370075\n",
      "loss: 6.392948\n",
      "loss: 6.944280\n",
      "loss: 6.356041\n",
      "loss: 6.456190\n",
      "loss: 6.633496\n",
      "loss: 6.684272\n",
      "loss: 6.509820\n",
      "loss: 6.349310\n",
      "loss: 6.015771\n",
      "loss: 6.163836\n",
      "loss: 6.275856\n",
      "loss: 6.678157\n",
      "loss: 6.245019\n",
      "loss: 6.556546\n",
      "loss: 5.988595\n",
      "loss: 6.176577\n",
      "loss: 6.266938\n",
      "loss: 5.961045\n",
      "loss: 6.337167\n",
      "loss: 6.379451\n",
      "loss: 6.635712\n",
      "loss: 6.179230\n",
      "loss: 6.171418\n",
      "loss: 6.156450\n",
      "loss: 6.500946\n",
      "loss: 6.150453\n",
      "loss: 6.260150\n",
      "loss: 6.167805\n",
      "loss: 6.236889\n",
      "loss: 6.246094\n",
      "loss: 6.389155\n",
      "loss: 6.141690\n",
      "loss: 6.588599\n",
      "loss: 6.217746\n",
      "loss: 6.134827\n",
      "loss: 6.183665\n",
      "loss: 6.217889\n",
      "loss: 6.180767\n",
      "loss: 6.286521\n",
      "loss: 6.212701\n",
      "loss: 6.417096\n",
      "loss: 6.532193\n",
      "loss: 5.979300\n",
      "loss: 6.481362\n",
      "loss: 6.362310\n",
      "loss: 6.525193\n",
      "loss: 6.653739\n",
      "loss: 6.736862\n",
      "loss: 6.335542\n",
      "loss: 6.316397\n",
      "loss: 6.270535\n",
      "loss: 6.396908\n",
      "loss: 6.611585\n",
      "loss: 6.344360\n",
      "loss: 6.375481\n",
      "loss: 6.773173\n",
      "loss: 6.261660\n",
      "loss: 6.247436\n",
      "loss: 6.378146\n",
      "loss: 6.340441\n",
      "loss: 6.232065\n",
      "loss: 6.359249\n",
      "loss: 6.157182\n",
      "loss: 6.673883\n",
      "loss: 6.225179\n",
      "loss: 6.120104\n",
      "loss: 6.600960\n",
      "loss: 6.598649\n",
      "loss: 6.562202\n",
      "loss: 6.383043\n",
      "loss: 6.226417\n",
      "loss: 6.729538\n",
      "loss: 6.362647\n",
      "loss: 6.280666\n",
      "loss: 6.262806\n",
      "loss: 6.336206\n",
      "loss: 5.903217\n",
      "loss: 6.433607\n",
      "loss: 6.544107\n",
      "loss: 6.457008\n",
      "loss: 6.354094\n",
      "loss: 5.659436\n",
      "loss: 6.265129\n",
      "loss: 6.310928\n",
      "loss: 6.413092\n",
      "loss: 6.580922\n",
      "loss: 6.390742\n",
      "loss: 6.359967\n",
      "loss: 6.319325\n",
      "loss: 6.401779\n",
      "loss: 6.226818\n",
      "loss: 6.310166\n",
      "loss: 6.297323\n",
      "loss: 6.434206\n",
      "loss: 6.452252\n",
      "loss: 6.364706\n",
      "loss: 6.083338\n",
      "loss: 6.281110\n",
      "loss: 6.573635\n",
      "loss: 6.009434\n",
      "loss: 6.479142\n",
      "loss: 6.381258\n",
      "loss: 6.678421\n",
      "loss: 6.317934\n",
      "loss: 6.142314\n",
      "loss: 5.985386\n",
      "loss: 6.132558\n",
      "loss: 6.352147\n",
      "loss: 6.219502\n",
      "loss: 6.363409\n",
      "loss: 6.251605\n",
      "loss: 6.349649\n",
      "loss: 5.915504\n",
      "loss: 5.999892\n",
      "loss: 6.352346\n",
      "loss: 6.310359\n",
      "loss: 6.173954\n",
      "loss: 6.360595\n",
      "loss: 6.537058\n",
      "loss: 6.402565\n",
      "loss: 6.277951\n",
      "loss: 6.489588\n",
      "loss: 6.498418\n",
      "loss: 5.803699\n",
      "loss: 6.308536\n",
      "loss: 6.222105\n",
      "loss: 5.984188\n",
      "loss: 6.067579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 6.552084\n",
      "loss: 5.805659\n",
      "loss: 6.380638\n",
      "loss: 6.515791\n",
      "loss: 6.319370\n",
      "loss: 6.559206\n",
      "loss: 6.476161\n",
      "loss: 6.117534\n",
      "loss: 6.170471\n",
      "loss: 6.270523\n",
      "loss: 6.398271\n",
      "loss: 6.335836\n",
      "loss: 6.407510\n",
      "loss: 6.468328\n",
      "loss: 6.322253\n",
      "loss: 6.627347\n",
      "loss: 6.447699\n",
      "loss: 6.006077\n",
      "loss: 6.163225\n",
      "loss: 6.569389\n",
      "loss: 6.571123\n",
      "loss: 6.350945\n",
      "loss: 6.225169\n",
      "loss: 6.168538\n",
      "loss: 6.491021\n",
      "loss: 6.069513\n",
      "loss: 6.352233\n",
      "loss: 5.965123\n",
      "loss: 6.042046\n",
      "loss: 6.468031\n",
      "loss: 6.096445\n",
      "loss: 6.130833\n",
      "loss: 6.352761\n",
      "loss: 6.418515\n",
      "loss: 6.353198\n",
      "loss: 6.394598\n",
      "loss: 6.425023\n",
      "loss: 6.503269\n",
      "loss: 6.076306\n",
      "loss: 6.265134\n",
      "loss: 6.436759\n",
      "loss: 5.957347\n",
      "loss: 6.480291\n",
      "loss: 6.475164\n",
      "loss: 6.333910\n",
      "loss: 6.826436\n",
      "loss: 6.085797\n",
      "loss: 6.519462\n",
      "loss: 6.667211\n",
      "loss: 6.477669\n",
      "loss: 5.848251\n",
      "loss: 6.280698\n",
      "loss: 6.527968\n",
      "loss: 6.408465\n",
      "loss: 6.586850\n",
      "loss: 6.279148\n",
      "loss: 6.368783\n",
      "loss: 6.185057\n",
      "loss: 6.586433\n",
      "loss: 5.599621\n",
      "loss: 6.236046\n",
      "loss: 6.170139\n",
      "loss: 6.609645\n",
      "loss: 6.417264\n",
      "loss: 6.403788\n",
      "loss: 6.373801\n",
      "loss: 6.321401\n",
      "loss: 6.187867\n",
      "loss: 6.431701\n",
      "loss: 6.488087\n",
      "loss: 5.936100\n",
      "loss: 5.864616\n",
      "loss: 6.220236\n",
      "loss: 6.637207\n",
      "loss: 6.219611\n",
      "loss: 6.373075\n",
      "loss: 6.217719\n",
      "loss: 6.303768\n",
      "loss: 6.442184\n",
      "loss: 6.467206\n",
      "loss: 6.234615\n",
      "loss: 6.684671\n",
      "loss: 6.401011\n",
      "loss: 6.341986\n",
      "loss: 5.934953\n",
      "loss: 6.112504\n",
      "loss: 6.086043\n",
      "loss: 6.300241\n",
      "loss: 6.652577\n",
      "loss: 6.045374\n",
      "loss: 6.175442\n",
      "loss: 6.158924\n",
      "loss: 6.375611\n",
      "loss: 6.371981\n",
      "loss: 6.586363\n",
      "loss: 6.126695\n",
      "loss: 6.429904\n",
      "loss: 6.537097\n",
      "loss: 5.944174\n",
      "loss: 6.351972\n",
      "loss: 6.252527\n",
      "loss: 6.269855\n",
      "loss: 6.096820\n",
      "loss: 6.248588\n",
      "loss: 6.236342\n",
      "loss: 6.108156\n",
      "loss: 6.079114\n",
      "loss: 6.226276\n",
      "loss: 6.224392\n",
      "loss: 5.895844\n",
      "loss: 5.982001\n",
      "loss: 6.245026\n",
      "loss: 6.221136\n",
      "loss: 6.526381\n",
      "loss: 6.297569\n",
      "loss: 6.551517\n",
      "loss: 6.495544\n",
      "loss: 6.755965\n",
      "loss: 6.244369\n",
      "loss: 6.217404\n",
      "loss: 6.338500\n",
      "loss: 6.209046\n",
      "loss: 6.037009\n",
      "loss: 6.237344\n",
      "loss: 6.314437\n",
      "loss: 6.456868\n",
      "loss: 6.670042\n",
      "loss: 6.138284\n",
      "loss: 6.829307\n",
      "loss: 6.575418\n",
      "loss: 6.713150\n",
      "loss: 6.098658\n",
      "loss: 6.365479\n",
      "loss: 6.376761\n",
      "loss: 6.334311\n",
      "loss: 6.046242\n",
      "loss: 6.366558\n",
      "loss: 6.115265\n",
      "loss: 6.458147\n",
      "loss: 6.090937\n",
      "loss: 5.856714\n",
      "loss: 6.116753\n",
      "loss: 6.233639\n",
      "loss: 6.346719\n",
      "loss: 6.495851\n",
      "loss: 5.881032\n",
      "loss: 6.278768\n",
      "loss: 6.400077\n",
      "loss: 6.240959\n",
      "loss: 5.849764\n",
      "loss: 6.245652\n",
      "loss: 6.339798\n",
      "loss: 6.431058\n",
      "loss: 6.272193\n",
      "loss: 6.312244\n",
      "loss: 6.608825\n",
      "loss: 6.161211\n",
      "loss: 6.275403\n",
      "loss: 6.620640\n",
      "loss: 6.272390\n",
      "loss: 6.384323\n",
      "loss: 6.010844\n",
      "loss: 5.879856\n",
      "loss: 6.407700\n",
      "loss: 6.362462\n",
      "loss: 6.107328\n",
      "loss: 6.193709\n",
      "loss: 6.167129\n",
      "loss: 6.603778\n",
      "loss: 6.600481\n",
      "loss: 6.178716\n",
      "loss: 6.534174\n",
      "loss: 6.268493\n",
      "loss: 6.289570\n",
      "loss: 6.136467\n",
      "loss: 6.143741\n",
      "loss: 6.393391\n",
      "loss: 6.154113\n",
      "loss: 6.407031\n",
      "loss: 6.241363\n",
      "loss: 5.654115\n",
      "loss: 6.355088\n",
      "loss: 6.368772\n",
      "loss: 6.306700\n",
      "loss: 6.503897\n",
      "loss: 6.388464\n",
      "loss: 6.332410\n",
      "loss: 6.260274\n",
      "loss: 6.285874\n",
      "loss: 5.843428\n",
      "loss: 6.155108\n",
      "loss: 6.174757\n",
      "loss: 6.549104\n",
      "loss: 6.463492\n",
      "loss: 6.455967\n",
      "loss: 6.245740\n",
      "loss: 6.079523\n",
      "loss: 6.627852\n",
      "loss: 6.440879\n",
      "loss: 6.524257\n",
      "loss: 5.974619\n",
      "loss: 6.419905\n",
      "loss: 6.002404\n",
      "loss: 6.534443\n",
      "loss: 6.233220\n",
      "loss: 6.318161\n",
      "loss: 6.079766\n",
      "loss: 6.200975\n",
      "loss: 6.097489\n",
      "loss: 6.215861\n",
      "loss: 6.015038\n",
      "loss: 6.222559\n",
      "loss: 6.265367\n",
      "loss: 6.486508\n",
      "loss: 6.380538\n",
      "loss: 6.195721\n",
      "loss: 6.519458\n",
      "loss: 6.126207\n",
      "loss: 6.130885\n",
      "loss: 6.255689\n",
      "loss: 5.995245\n",
      "loss: 6.247622\n",
      "loss: 6.217727\n",
      "loss: 6.667674\n",
      "loss: 6.417693\n",
      "loss: 5.935461\n",
      "loss: 6.028483\n",
      "loss: 6.504218\n",
      "loss: 6.206308\n",
      "loss: 6.243331\n",
      "loss: 5.946897\n",
      "loss: 6.123423\n",
      "loss: 6.364870\n",
      "loss: 6.160890\n",
      "loss: 6.419925\n",
      "loss: 5.777043\n",
      "loss: 5.950238\n",
      "loss: 6.375788\n",
      "loss: 6.474352\n",
      "loss: 6.268793\n",
      "loss: 6.335059\n",
      "loss: 6.655173\n",
      "loss: 6.489320\n",
      "loss: 6.561404\n",
      "loss: 6.200261\n",
      "loss: 6.161105\n",
      "loss: 6.154725\n",
      "loss: 6.371799\n",
      "loss: 5.956590\n",
      "loss: 6.169912\n",
      "loss: 6.121062\n",
      "loss: 6.642167\n",
      "loss: 6.284133\n",
      "loss: 6.215550\n",
      "loss: 6.315616\n",
      "loss: 6.475471\n",
      "loss: 6.414986\n",
      "loss: 6.441772\n",
      "loss: 6.535692\n",
      "loss: 6.098172\n",
      "loss: 6.168388\n",
      "loss: 6.371359\n",
      "loss: 6.078882\n",
      "loss: 6.267950\n",
      "loss: 5.984094\n",
      "loss: 6.099070\n",
      "loss: 5.558183\n",
      "loss: 6.612127\n",
      "loss: 6.035062\n",
      "loss: 6.459995\n",
      "loss: 6.213719\n",
      "loss: 5.910485\n",
      "loss: 6.128556\n",
      "loss: 6.130844\n",
      "loss: 6.337609\n",
      "loss: 6.604592\n",
      "loss: 6.518648\n",
      "loss: 6.206590\n",
      "loss: 6.196559\n",
      "loss: 6.190585\n",
      "loss: 6.241464\n",
      "loss: 6.492630\n",
      "loss: 6.368545\n",
      "loss: 5.965276\n",
      "loss: 5.811511\n",
      "loss: 6.203636\n",
      "loss: 6.485116\n",
      "loss: 6.420417\n",
      "loss: 6.387416\n",
      "loss: 6.479496\n",
      "loss: 6.141298\n",
      "loss: 6.205761\n",
      "loss: 6.206343\n",
      "loss: 6.173016\n",
      "loss: 6.452259\n",
      "loss: 6.031707\n",
      "loss: 6.299821\n",
      "loss: 6.683536\n",
      "loss: 6.234959\n",
      "loss: 6.249486\n",
      "loss: 6.373376\n",
      "loss: 6.024236\n",
      "loss: 6.004402\n",
      "loss: 6.199847\n",
      "loss: 6.415986\n",
      "loss: 6.248296\n",
      "loss: 6.345051\n",
      "loss: 6.344458\n",
      "loss: 5.985488\n",
      "loss: 6.393272\n",
      "loss: 6.143289\n",
      "loss: 6.475189\n",
      "loss: 6.401803\n",
      "loss: 6.514073\n",
      "loss: 6.334170\n",
      "loss: 6.368370\n",
      "loss: 6.295328\n",
      "loss: 6.086283\n",
      "loss: 6.544066\n",
      "loss: 6.070635\n",
      "loss: 6.285688\n",
      "loss: 6.413976\n",
      "loss: 6.234547\n",
      "loss: 6.554445\n",
      "loss: 5.900147\n",
      "loss: 6.264193\n",
      "loss: 6.359750\n",
      "loss: 6.689061\n",
      "loss: 6.274119\n",
      "loss: 6.237138\n",
      "loss: 6.378770\n",
      "loss: 6.163166\n",
      "loss: 6.248542\n",
      "loss: 6.305111\n",
      "loss: 6.198775\n",
      "loss: 6.439049\n",
      "loss: 6.333395\n",
      "loss: 5.981924\n",
      "loss: 6.406615\n",
      "loss: 6.472935\n",
      "loss: 6.210157\n",
      "loss: 6.211626\n",
      "loss: 6.434516\n",
      "loss: 6.383525\n",
      "loss: 6.142884\n",
      "loss: 6.266618\n",
      "loss: 6.394395\n",
      "loss: 5.902870\n",
      "loss: 6.230189\n",
      "loss: 6.014486\n",
      "loss: 6.342597\n",
      "loss: 6.521134\n",
      "loss: 6.338237\n",
      "loss: 6.630412\n",
      "loss: 6.241477\n",
      "loss: 6.765678\n",
      "loss: 6.561660\n",
      "loss: 6.016171\n",
      "loss: 5.902193\n",
      "loss: 6.001792\n",
      "loss: 6.390751\n",
      "loss: 6.359660\n",
      "loss: 6.215725\n",
      "loss: 6.279324\n",
      "loss: 6.740629\n",
      "loss: 6.592838\n",
      "loss: 6.391560\n",
      "loss: 6.586135\n",
      "loss: 6.478161\n",
      "loss: 6.264528\n",
      "loss: 6.387123\n",
      "loss: 6.421370\n",
      "loss: 6.322822\n",
      "loss: 6.308860\n",
      "loss: 6.540233\n",
      "loss: 6.100708\n",
      "loss: 6.155898\n",
      "loss: 6.514631\n",
      "loss: 6.396208\n",
      "loss: 6.065512\n",
      "loss: 6.293227\n",
      "loss: 6.379092\n",
      "loss: 6.333463\n",
      "loss: 6.314656\n",
      "loss: 6.281009\n",
      "loss: 6.432021\n",
      "loss: 6.567296\n",
      "loss: 6.737960\n",
      "loss: 6.395800\n",
      "loss: 6.388788\n",
      "loss: 6.505360\n",
      "loss: 6.176716\n",
      "loss: 6.324649\n",
      "loss: 6.177362\n",
      "loss: 6.046124\n",
      "loss: 6.446537\n",
      "loss: 6.315103\n",
      "loss: 6.585322\n",
      "loss: 5.833192\n",
      "loss: 6.468872\n",
      "loss: 6.365450\n",
      "loss: 6.650437\n",
      "loss: 6.054001\n",
      "loss: 6.037141\n",
      "loss: 6.552761\n",
      "loss: 6.538587\n",
      "loss: 6.360090\n",
      "loss: 6.052751\n",
      "loss: 6.258543\n",
      "loss: 6.340536\n",
      "loss: 6.456229\n",
      "loss: 6.395093\n",
      "loss: 5.884438\n",
      "loss: 6.818048\n",
      "loss: 6.049758\n",
      "loss: 6.540522\n",
      "loss: 6.299867\n",
      "loss: 6.083637\n",
      "loss: 6.263524\n",
      "loss: 6.410262\n",
      "loss: 6.186509\n",
      "loss: 6.034081\n",
      "loss: 6.522910\n",
      "loss: 6.504199\n",
      "loss: 6.152514\n",
      "loss: 6.581038\n",
      "loss: 6.311272\n",
      "loss: 6.335620\n",
      "loss: 6.275208\n",
      "loss: 6.444962\n",
      "loss: 6.327803\n",
      "loss: 6.294844\n",
      "loss: 6.019494\n",
      "loss: 6.148362\n",
      "loss: 6.275305\n",
      "loss: 6.006328\n",
      "loss: 5.901127\n",
      "loss: 6.209736\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-f0a18b357661>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.002\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m#train(encoder, decoder, data, loss_fn, optimizer, config.batch_size, config.num_steps)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-162-c63a550ad369>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(encoder, decoder, data, loss_fn, optimizer, batch_size, num_steps)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m#print(\"target_length in this batch: %d\" % target_length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mdecoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_hidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m             \u001b[0mtopv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0mdecoder_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-156-22c82dbf5777>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_token, hidden)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config = Config()\n",
    "encoder = Encoder(config)\n",
    "decoder = Decoder(config)\n",
    "# data is defined on top\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "loss_fn = nn.NLLLoss(size_average=False, ignore_index=config.padding_idx)\n",
    "optimizer = optim.Adam(params, lr=0.002)\n",
    "#train(encoder, decoder, data, loss_fn, optimizer, config.batch_size, config.num_steps)\n",
    "train(encoder, decoder, data, loss_fn, optimizer, batch_size=30, num_steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2Seq(nn.Module):\n",
    "    def __init__(self, config, criterion):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, rating, category, tag, target_tensor):\n",
    "        # 함수 호출시 *[rating, category, tag]하기!\n",
    "\n",
    "        batch_size = target_tensor.size(0)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        attr, encoder_output = self.encoder(rating,category,tag)\n",
    "        batch_size = encoder_output.size(0)\n",
    "        decoder_hidden = self.splitHidden(encoder_output)\n",
    "        input_token = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        \n",
    "        decoder_outputs = []\n",
    "        for idx in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(input_token, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input_token = topi.detach().view(batch_size, 1)\n",
    "            decoder_outputs.append(decoder_output)\n",
    "\n",
    "        # 이 아래 두 개 shape 조정은 loss function 요구 사항 맞추기 위함!\n",
    "        decoder_outputs = torch.cat(decoder_outputs, 1).view(batch_size*target_length, -1)\n",
    "        target_tensor = target_tensor.view(-1)\n",
    "        loss = self.criterion(decoder_outputs, target_tensor) #/ batch_size\n",
    "                        # batch_size로 나눠주기\n",
    "#         print(\"decoder_outputs: \", decoder_outputs)\n",
    "#         print(\"target_tensor: \", target_tensor)\n",
    "        return loss\n",
    "    \n",
    "    def splitHidden(self, encoder_output):\n",
    "        h_0 = encoder_output.view(self.config.num_layers, encoder_output.size(0), \\\n",
    "                                  self.config.hidden_size)\n",
    "        c_0 = torch.zeros_like(h_0) \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def inference(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== testing Attr2Seq model... ====\n",
      "tensor(10.0135)\n",
      "\n",
      "==== baching(size 2) ====\n",
      "tensor(10.0085)\n"
     ]
    }
   ],
   "source": [
    "print(\"==== testing Attr2Seq model... ====\")\n",
    "criterion = nn.NLLLoss()     # TODO: padding loss빼주기ㅠㅠ\n",
    "model = Attr2Seq(config, criterion)\n",
    "input_list = [torch.tensor([[2]]).type(torch.long), torch.tensor([[7]]).type(torch.long),\n",
    "              torch.tensor([[0,2,1]])]\n",
    "target_tensor = torch.tensor([[4]])\n",
    "loss = model(*input_list, target_tensor)\n",
    "print(loss)\n",
    "\n",
    "print(\"\\n==== baching(size 2) ====\")\n",
    "input_list = [torch.tensor([[2],[1]]).type(torch.long), torch.tensor([[7],[5]]).type(torch.long),\n",
    "              torch.tensor([[0,2,1],[2,0,1]])]\n",
    "target_tensor = torch.tensor([[9,2,1,3,5], [3,2,3,2,1]])\n",
    "loss = model(*input_list, target_tensor)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 전에 Train / Dev 나누기\n",
    "# TODO: data랑 받아서 정의된 data iterator로 불러오기\n",
    "\n",
    "def train(data, model, optimizer, num_steps):\n",
    "    model.train()\n",
    "\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "#        a = list(model.parameters())[0]\n",
    "        optimizer.zero_grad()\n",
    "        r, c, t, rv = data_iterator(data, batch_size = 2)\n",
    "        loss = model(r, c, t, rv)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()        \n",
    "#         for name, param in model.named_parameters():\n",
    "#             print(name)\n",
    "#             print(param.grad)\n",
    "#        b = list(model.parameters())[0]\n",
    "#        print(loss)\n",
    "    return loss.item()# / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "081668a859a54593ae6165a8fc927079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "nan\n"
     ]
    }
   ],
   "source": [
    "model = Attr2Seq(config, criterion)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\n",
    "temp_loss = train(Encoded, model, optimizer, 2)\n",
    "print(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building vocab for tags ...\n",
      "Building vocab for reviews ...\n",
      "3339 out of 3465 words left, which is 96.36363636363636 %\n",
      "22193 out of 39173 words left, which is 56.653817680545274 %\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95224f9bdd434327bd9ca3f29f5c4e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=184342), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attr2Seq(nn.Module):\n",
    "    def __init__(self, config, criterion):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.criterion = criterion\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "    \n",
    "    def forward(self, rating, category, tag, target_tensor):\n",
    "\n",
    "        batch_size = target_tensor.size(0)\n",
    "        target_length = target_tensor.size(-1)\n",
    "        attr, encoder_output = self.encoder(rating,category,tag)\n",
    "        batch_size = encoder_output.size(0)\n",
    "        decoder_hidden = self.splitHidden(encoder_output)\n",
    "        input_token = torch.zeros((batch_size,1)).type(torch.long)    # SOS token\n",
    "        \n",
    "        loss = 0\n",
    "        for idx in range(target_length): \n",
    "            decoder_output, decoder_hidden = decoder(input_token, decoder_hidden)            \n",
    "            topv, topi = decoder_output.topk(1)\n",
    "            input_token = topi.detach().view(batch_size, 1)\n",
    "            loss += self.criterion(decoder_output.squeeze(), target_tensor[:,idx].view(-1))\n",
    "\n",
    "        loss = loss / batch_size\n",
    "        return loss\n",
    "    \n",
    "    def splitHidden(self, encoder_output):\n",
    "        h_0 = encoder_output.view(self.config.num_layers, encoder_output.size(0), \\\n",
    "                                  self.config.hidden_size)\n",
    "        c_0 = torch.zeros_like(h_0) \n",
    "        return (h_0, c_0)\n",
    "    \n",
    "    def inference(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 그 전에 Train / Dev 나누기\n",
    "# TODO: data랑 받아서 정의된 data iterator로 불러오기\n",
    "\n",
    "def train(data, model, criterion, optimizer, num_steps):\n",
    "    model.train()\n",
    "    loss_fn = criterion\n",
    "    for t in tqdm_notebook(range(num_steps)):\n",
    "        a = list(model.parameters())[0]\n",
    "        optimizer.zero_grad()\n",
    "        r, c, t, rv = data_iterator(data, batch_size = 2)\n",
    "            \n",
    "        loss = model(r, c, t, rv)\n",
    "#        decoder_output = model(r, c, t, rv)\n",
    "#        rv = rv.view(-1)\n",
    "#        loss = loss_fn(decoder_output, rv)\n",
    "        loss.backward()\n",
    "#        print(loss.grad_fn)\n",
    "        print(loss)\n",
    "        \n",
    "        #nn.utils.clip_grad_norm_(model.parameters(), 0.25)\n",
    "        optimizer.step()        \n",
    "        for name, param in model.named_parameters():\n",
    "            print(name)\n",
    "            print(param.grad)\n",
    "        b = list(model.parameters())[0]\n",
    "        \n",
    "        print(loss)\n",
    "    return loss.item()# / target_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c508628787c043a888c6e378216ae8d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=10), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-22:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/tqdm/_monitor.py\", line 62, in run\n",
      "    for instance in self.tqdm_cls._instances:\n",
      "  File \"/home/nlp908/anaconda3/envs/hwijeen_3.6/lib/python3.6/_weakrefset.py\", line 60, in __iter__\n",
      "    for itemref in self.data:\n",
      "RuntimeError: Set changed size during iteration\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(239.8178)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-0.1394, -0.4734,  0.0901, -0.2424, -0.0275, -0.2197,  0.5335,\n",
      "         -0.1829, -0.4313,  0.5435,  0.6294,  0.0810,  0.1318,  0.9710,\n",
      "          0.2163,  0.3097,  0.1586,  0.3123, -1.0216, -0.0940,  0.0342,\n",
      "         -0.6293,  0.1568,  0.3926,  0.5302,  0.0847, -0.0176,  0.0282,\n",
      "          0.2373, -0.5901, -0.1565, -0.1280, -0.2264,  0.3513, -0.2850,\n",
      "          0.4641, -0.4173,  0.2349, -0.0013,  0.4252,  0.3957, -0.0341,\n",
      "          0.0350,  0.4284, -0.2735,  0.2149,  0.2342, -0.4443,  0.1432,\n",
      "         -0.4402, -0.3124, -0.2952, -0.2387,  0.3001,  0.0590, -0.1819,\n",
      "         -0.1179, -0.0944,  0.2338,  0.3593, -0.2178, -0.0130,  0.3182,\n",
      "         -0.2979],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-1.3002, -1.3762,  0.6344,  0.2776, -1.3070, -1.4853, -0.5950,\n",
      "          0.2295, -0.2682,  1.6754, -1.7311, -1.4065,  0.7270,  0.4997,\n",
      "         -1.3453,  1.4607,  1.5160, -0.5727,  1.4798, -0.6808,  3.9223,\n",
      "          0.6346, -1.8802,  1.7132,  2.5296, -0.6179, -1.4657, -0.1866,\n",
      "         -0.0342,  0.9944,  3.4486,  0.4888, -0.1171,  0.2087, -0.5427,\n",
      "          0.2740, -0.9423, -0.9543, -1.2520,  2.6885, -0.6509, -0.1169,\n",
      "          0.2064, -1.6798, -1.3745, -0.3588, -1.2679,  0.4022,  0.5690,\n",
      "          3.2051, -1.6332, -2.4246,  0.0360, -0.0170,  1.2612, -0.5281,\n",
      "         -1.1087,  1.5464, -3.2279,  2.1084, -0.7763,  0.5661, -1.3028,\n",
      "         -0.9439],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.4094, -0.2905,  0.0694,  ..., -0.0651,  0.0056,  0.1072],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[ 3.0455e-05,  6.4547e-06,  3.9626e-05,  ...,  1.7587e-05,\n",
      "         -2.8142e-06,  9.0993e-05],\n",
      "        [-8.5778e-05, -1.6131e-05, -1.1409e-04,  ..., -4.9618e-05,\n",
      "          6.8098e-06, -2.5871e-04],\n",
      "        [ 1.2599e-05,  8.8920e-06,  8.8665e-06,  ...,  7.0212e-06,\n",
      "         -4.5551e-06,  3.0283e-05],\n",
      "        ...,\n",
      "        [-7.0538e-05, -1.7491e-05, -8.8704e-05,  ..., -4.0630e-05,\n",
      "          7.9032e-06, -2.0774e-04],\n",
      "        [-8.3883e-05, -1.7260e-05, -1.0977e-04,  ..., -4.8462e-05,\n",
      "          7.4685e-06, -2.5124e-04],\n",
      "        [-4.8691e-05, -1.4112e-05, -5.8766e-05,  ..., -2.7963e-05,\n",
      "          6.5661e-06, -1.4099e-04]])\n",
      "encoder.out.bias\n",
      "tensor([-5.4033e-04,  1.4847e-03, -3.3619e-04,  ...,  1.2975e-03,\n",
      "         1.4788e-03,  9.3254e-04])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(239.8178)\n",
      "tensor(184.9530)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[-1.3474, -1.7485, -0.5939,  0.9072,  0.2683, -1.3788,  1.2970,\n",
      "          1.5173, -0.4313,  0.4661, -0.2176,  0.3852,  0.7642, -0.5767,\n",
      "         -0.6824, -1.1337,  1.1761, -0.8963, -1.1321, -1.6096,  3.0573,\n",
      "          1.9313, -1.0760,  2.7680,  2.6189,  0.3410, -1.9148, -1.9158,\n",
      "          1.7822, -0.2703,  1.7105,  1.4060, -0.1042,  1.3080, -2.1033,\n",
      "          0.2506,  1.2573, -0.0276,  0.1767,  3.0383, -1.7698,  1.8860,\n",
      "         -0.7465, -0.4451, -3.5081,  0.8240, -2.0428,  0.0684,  2.8931,\n",
      "          2.6074, -0.4980, -0.6003, -0.1587,  2.3374,  1.8680, -0.9444,\n",
      "         -4.2063,  0.5632, -1.5759,  2.5300,  1.2959,  1.8044, -2.0168,\n",
      "          0.1234],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-04 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[-3.2210e-06, -1.7455e-05,  8.6184e-06,  ..., -1.9152e-06,\n",
      "         -1.6223e-06, -9.1215e-07],\n",
      "        [ 2.2034e-05,  1.1940e-04, -5.8955e-05,  ...,  1.1937e-05,\n",
      "          1.4321e-05,  3.3050e-06],\n",
      "        [-6.0278e-06, -3.2665e-05,  1.6129e-05,  ..., -5.4628e-06,\n",
      "          2.1651e-06, -6.4425e-06],\n",
      "        ...,\n",
      "        [ 1.5155e-05,  8.2124e-05, -4.0550e-05,  ...,  7.2880e-06,\n",
      "          1.2403e-05, -5.1685e-08],\n",
      "        [ 2.7186e-05,  1.4732e-04, -7.2742e-05,  ...,  1.8101e-05,\n",
      "          8.3336e-06,  1.2578e-05],\n",
      "        [ 3.6138e-05,  1.9583e-04, -9.6695e-05,  ...,  2.4279e-05,\n",
      "          1.0475e-05,  1.7269e-05]])\n",
      "encoder.out.bias\n",
      "tensor(1.00000e-03 *\n",
      "       [-0.2292,  1.5676, -0.4288,  ...,  1.0782,  1.9342,  2.5711])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(184.9530)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(55.0357)\n",
      "encoder.emb_rating.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000],\n",
      "        [-1.7598, -0.9653, -1.0876, -0.9319, -0.5623, -1.0171,  1.0912,\n",
      "          2.0564, -2.1061,  1.1830,  0.0069, -0.3403,  0.8550,  0.0446,\n",
      "         -2.6909,  0.3595, -0.0877, -1.1301, -1.1464, -1.3937,  3.3239,\n",
      "          0.6338, -2.7224,  1.8870,  1.4685,  0.1902, -1.5925, -1.9791,\n",
      "         -0.7989,  0.2858,  3.5813,  0.2917, -1.2962, -1.0036, -2.0289,\n",
      "          0.5360,  0.5821, -1.4962,  0.2813,  3.5992, -0.0155,  1.5918,\n",
      "          0.3996, -0.1368, -3.4686,  1.0226, -1.9875, -0.5433, -0.2208,\n",
      "          0.5349, -0.5199, -0.8176,  1.5497,  1.5824,  2.7518,  0.1363,\n",
      "         -2.8182,  2.8445, -1.7713,  1.0375,  1.6412,  0.9582, -1.5797,\n",
      "         -1.0454],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000]])\n",
      "encoder.emb_category.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.3429,  0.2823,  0.5932,  ...,  0.3328, -0.5151,  0.1138],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.emb_tag.weight\n",
      "tensor(1.00000e-03 *\n",
      "       [[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "encoder.out.weight\n",
      "tensor([[ 4.8431e-05,  1.1360e-05,  6.2604e-05,  ...,  1.0180e-05,\n",
      "          1.3778e-07,  2.2405e-05],\n",
      "        [-5.3447e-05, -1.2537e-05, -6.9087e-05,  ..., -1.0665e-05,\n",
      "          6.3826e-08, -2.4007e-05],\n",
      "        [ 3.7661e-05,  8.8339e-06,  4.8682e-05,  ...,  8.3262e-06,\n",
      "          2.6250e-07,  1.7939e-05],\n",
      "        ...,\n",
      "        [-2.4385e-05, -5.7198e-06, -3.1521e-05,  ..., -5.4601e-06,\n",
      "         -1.9613e-07, -1.1702e-05],\n",
      "        [-5.0419e-05, -1.1827e-05, -6.5174e-05,  ..., -9.6141e-06,\n",
      "          2.2965e-07, -2.2084e-05],\n",
      "        [-1.3981e-04, -3.2794e-05, -1.8072e-04,  ..., -2.4601e-05,\n",
      "          1.4168e-06, -5.8645e-05]])\n",
      "encoder.out.bias\n",
      "tensor([-9.1859e-04,  1.0137e-03, -7.1432e-04,  ...,  4.6251e-04,\n",
      "         9.5630e-04,  2.6517e-03])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(55.0357)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.]])\n",
      "encoder.out.bias\n",
      "tensor([nan., nan., nan.,  ..., nan., nan., nan.])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(nan.)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "tensor([[nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.]])\n",
      "encoder.out.bias\n",
      "tensor([nan., nan., nan.,  ..., nan., nan., nan.])\n",
      "decoder.embedding.weight\n",
      "None\n",
      "decoder.lstm.weight_ih_l0\n",
      "None\n",
      "decoder.lstm.weight_hh_l0\n",
      "None\n",
      "decoder.lstm.bias_ih_l0\n",
      "None\n",
      "decoder.lstm.bias_hh_l0\n",
      "None\n",
      "decoder.lstm.weight_ih_l1\n",
      "None\n",
      "decoder.lstm.weight_hh_l1\n",
      "None\n",
      "decoder.lstm.bias_ih_l1\n",
      "None\n",
      "decoder.lstm.bias_hh_l1\n",
      "None\n",
      "decoder.out.weight\n",
      "None\n",
      "decoder.out.bias\n",
      "None\n",
      "tensor(nan.)\n",
      "tensor(nan.)\n",
      "encoder.emb_rating.weight\n",
      "tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,\n",
      "           0.,   0.,   0.,   0.],\n",
      "        [nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan., nan., nan., nan., nan., nan., nan.,\n",
      "         nan., nan., nan., nan.]])\n",
      "encoder.emb_category.weight\n",
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.emb_tag.weight\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [nan., nan., nan.,  ..., nan., nan., nan.],\n",
      "        ...,\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,  ...,   0.,   0.,   0.]])\n",
      "encoder.out.weight\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-72cdc8c4a3b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtemp_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEncoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-60-28dae72990bd>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(data, model, criterion, optimizer, num_steps)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;31m# characters to replace unicode characters with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensor_str\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'encoding'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_str\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0msuffix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m', dtype='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_number_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mscale\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m             \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mSCALE_FORMAT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/hwijeen_3.6/lib/python3.6/site-packages/torch/_tensor_str.py\u001b[0m in \u001b[0;36m_number_format\u001b[0;34m(tensor, min_sz)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;31m# TODO: use fmod?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m             \u001b[0mint_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "model = Attr2Seq(config, criterion)\n",
    "params = list(model.encoder.parameters()) + list(model.decoder.parameters())\n",
    "#optimizer = optim.SGD(model.parameters(), lr=1)\n",
    "optimizer = optim.SGD(params, lr=1)\n",
    "\n",
    "#optimizer = optim.RMSprop(model.parameters(), lr=0.2, alpha=0.95)\n",
    "temp_loss = train(Encoded, model, criterion, optimizer, 10)\n",
    "print(temp_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:hwijeen_3.6]",
   "language": "python",
   "name": "conda-env-hwijeen_3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
